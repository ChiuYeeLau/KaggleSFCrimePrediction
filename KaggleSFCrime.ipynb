{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "_author__ = 'Monkey'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# import matplotlib.pylab as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from matplotlib.colors import LogNorm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from copy import deepcopy\n",
    "import pdb\n",
    "\n",
    "import csv\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# import keras\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(fn):\n",
    "  data = []\n",
    "  with open(fn) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data = [row for row in reader]\n",
    "  return data\n",
    "\n",
    "label_train = get_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyiliu/anaconda/lib/python2.7/site-packages/keras/utils/np_utils.py:14: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y = np.zeros((len(y), nb_classes))\n",
      "/Users/xiaoyiliu/anaconda/lib/python2.7/site-packages/keras/utils/np_utils.py:16: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y[i, y[i]] = 1.\n"
     ]
    }
   ],
   "source": [
    "labels = 'ARSON,ASSAULT,BAD CHECKS,BRIBERY,BURGLARY,DISORDERLY CONDUCT,DRIVING UNDER THE INFLUENCE,DRUG/NARCOTIC,DRUNKENNESS,EMBEZZLEMENT,EXTORTION,FAMILY OFFENSES,FORGERY/COUNTERFEITING,FRAUD,GAMBLING,KIDNAPPING,LARCENY/THEFT,LIQUOR LAWS,LOITERING,MISSING PERSON,NON-CRIMINAL,OTHER OFFENSES,PORNOGRAPHY/OBSCENE MAT,PROSTITUTION,RECOVERED VEHICLE,ROBBERY,RUNAWAY,SECONDARY CODES,SEX OFFENSES FORCIBLE,SEX OFFENSES NON FORCIBLE,STOLEN PROPERTY,SUICIDE,SUSPICIOUS OCC,TREA,TRESPASS,VANDALISM,VEHICLE THEFT,WARRANTS,WEAPON LAWS'.split(',')\n",
    "\n",
    "label_fields = {'Category': lambda x: labels.index(x.replace(',', ''))}\n",
    "\n",
    "def get_fields(data, fields):\n",
    "  extracted = []\n",
    "  for row in data:\n",
    "    extract = []\n",
    "    for field, f in sorted(fields.items()):\n",
    "      info = f(row[field])\n",
    "      if type(info) == list:\n",
    "        extract.extend(info)\n",
    "      else:\n",
    "        extract.append(info)\n",
    "    extracted.append(np.array(extract, dtype=np.float32))\n",
    "  return extracted\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "labels_int = np.array(get_fields(label_train, label_fields))\n",
    "# pdb.set_trace()\n",
    "# labels_int = np_utils.to_categorical(labels_int)\n",
    "\n",
    "labels_cmp = np_utils.to_categorical(labels_int.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy_scaler = preprocessing.StandardScaler()\n",
    "xy_scaler.fit(train[[\"X\",\"Y\"]])\n",
    "train[[\"X\",\"Y\"]] = xy_scaler.transform(train[[\"X\",\"Y\"]])\n",
    "train = train[abs(train[\"Y\"]) < 100]\n",
    "train.index = range(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addressFeats = list(train.Address.value_counts()[:50].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractAddressFeatures(x, addressFeats):\n",
    "    if x not in addressFeats: return -1\n",
    "    else: return addressFeats.index(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_time (x):\n",
    "    DD = datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    time = DD.hour\n",
    "    day = DD.day\n",
    "    month = DD.month\n",
    "    year = DD.year\n",
    "    return time, day, month, year\n",
    "\n",
    "def get_season (x):\n",
    "    summer = 0\n",
    "    fall = 0\n",
    "    winter = 0\n",
    "    spring = 0\n",
    "    if (x in [5, 6, 7]):\n",
    "        summer = 1\n",
    "    if (x in [8, 9, 10]):\n",
    "        fall = 1\n",
    "    if (x in [11, 0, 1]):\n",
    "        winter = 1\n",
    "    if (x in [2, 3, 4]):\n",
    "        spring = 1\n",
    "    return summer, fall, winter, spring\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "## replace address by log odds\n",
    "\n",
    "def parse_data (df, logodds, logoddsPA):\n",
    "    feature_list = df.columns.tolist()\n",
    "    if \"Descript\" in feature_list:\n",
    "        feature_list.remove(\"Descript\")\n",
    "    if \"Resolution\" in feature_list:\n",
    "        feature_list.remove(\"Resolution\")\n",
    "    if \"Category\" in feature_list:\n",
    "        feature_list.remove(\"Category\")\n",
    "    if \"Id\" in feature_list:\n",
    "        feature_list.remove(\"Id\")\n",
    "    clean_Data = df[feature_list]\n",
    "    clean_Data.index = range(len(df))\n",
    "    print(\"Creating address features\")\n",
    "    address_features = clean_Data[\"Address\"].apply(lambda x: logodds[x])\n",
    "    # pdb.set_trace()\n",
    "    address_features.columns = [\"logodds\" + str(x) for x in range(len(address_features.columns))]\n",
    "    print(\"Parsing dates\")\n",
    "    clean_Data[\"Time\"], clean_Data[\"Day\"], clean_Data[\"Month\"], clean_Data[\"Year\"] = zip(*clean_Data[\"Dates\"].apply(parse_time))\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    print(\"Creating one-hot variables\")\n",
    "    \n",
    "    clean_Data['AddrImportance'] = clean_Data['Address'].apply(lambda x: extractAddressFeatures(x, addressFeats))\n",
    "    dummy_importance = pd.get_dummies(clean_Data['AddrImportance'], prefix = 'Address_Importance')\n",
    "    dummy_ranks_PD = pd.get_dummies(clean_Data['PdDistrict'], prefix = 'PD')\n",
    "    dummy_ranks_DAY = pd.get_dummies(clean_Data[\"DayOfWeek\"], prefix = 'DAY')\n",
    "    clean_Data[\"IsInterection\"] = clean_Data[\"Address\"].apply(lambda x: 1 if \"/\" in x else 0)\n",
    "    clean_Data[\"logoddsPA\"] = clean_Data[\"Address\"].apply(lambda x: logoddsPA[x])\n",
    "    print(\"droping processed columns\")\n",
    "    clean_Data = clean_Data.drop(\"PdDistrict\",axis = 1)\n",
    "    clean_Data = clean_Data.drop(\"DayOfWeek\",axis = 1)\n",
    "    \n",
    "    clean_Data = clean_Data.drop(\"Address\",axis = 1)\n",
    "    clean_Data = clean_Data.drop(\"AddrImportance\",axis = 1)\n",
    "    clean_Data = clean_Data.drop(\"Dates\",axis = 1)\n",
    "    feature_list = clean_Data.columns.tolist()\n",
    "    print(\"joining one-hot features\")\n",
    "    features = clean_Data[feature_list].join(dummy_ranks_PD.ix[:,:]).join(dummy_ranks_DAY.ix[:,:]).join(address_features.ix[:,:]).join(dummy_importance.ix[:,:])\n",
    "    print(\"creating new features\")\n",
    "    features[\"IsDup\"] = pd.Series(features.duplicated()|features.duplicated(take_last = True)).apply(int)\n",
    "    features[\"Awake\"] = features[\"Time\"].apply(lambda x: 1 if (x==0 or (x>=8 and x<=23)) else 0)\n",
    "    features[\"Summer\"], features[\"Fall\"], features[\"Winter\"], features[\"Spring\"] = zip(*features[\"Month\"].apply(get_season))\n",
    "    if \"Category\" in df.columns:\n",
    "        labels = df[\"Category\"].astype(pd.core.categorical.Categorical)\n",
    "    else:\n",
    "        labels = None\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses = sorted(train[\"Address\"].unique())\n",
    "categories = sorted(train[\"Category\"].unique())\n",
    "C_counts = train.groupby([\"Category\"]).size()\n",
    "A_C_counts = train.groupby([\"Address\",\"Category\"]).size()\n",
    "A_counts = train.groupby([\"Address\"]).size()\n",
    "logodds = {}\n",
    "logoddsPA = {}\n",
    "MIN_CAT_COUNTS = 2\n",
    "default_logodds = np.log(C_counts/len(train)) - np.log(1.0-C_counts/float(len(train)))\n",
    "\n",
    "for addr in addresses:\n",
    "    PA = A_counts[addr]/float(len(train))\n",
    "    logoddsPA[addr] = np.log(PA)-np.log(1.-PA)\n",
    "    logodds[addr] = deepcopy(default_logodds)\n",
    "    for cat in A_C_counts[addr].keys():\n",
    "        if (A_C_counts[addr][cat] > MIN_CAT_COUNTS) and A_C_counts[addr][cat] < A_counts[addr]:\n",
    "            PA = A_C_counts[addr][cat]/float(A_counts[addr])\n",
    "            logodds[addr][categories.index(cat)] = np.log(PA) - np.log(1.0-PA)\n",
    "    logodds[addr] = pd.Series(logodds[addr])\n",
    "    logodds[addr].index = range(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating address features\n",
      "Parsing dates\n",
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n"
     ]
    }
   ],
   "source": [
    "features, labels = parse_data(train, logodds, logoddsPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'X', u'Y', u'Time', u'Day', u'Month', u'Year', u'IsInterection', u'logoddsPA', u'PD_BAYVIEW', u'PD_CENTRAL', u'PD_INGLESIDE', u'PD_MISSION', u'PD_NORTHERN', u'PD_PARK', u'PD_RICHMOND', u'PD_SOUTHERN', u'PD_TARAVAL', u'PD_TENDERLOIN', u'DAY_Friday', u'DAY_Monday', u'DAY_Saturday', u'DAY_Sunday', u'DAY_Thursday', u'DAY_Tuesday', u'DAY_Wednesday', u'logodds0', u'logodds1', u'logodds2', u'logodds3', u'logodds4', u'logodds5', u'logodds6', u'logodds7', u'logodds8', u'logodds9', u'logodds10', u'logodds11', u'logodds12', u'logodds13', u'logodds14', u'logodds15', u'logodds16', u'logodds17', u'logodds18', u'logodds19', u'logodds20', u'logodds21', u'logodds22', u'logodds23', u'logodds24', u'logodds25', u'logodds26', u'logodds27', u'logodds28', u'logodds29', u'logodds30', u'logodds31', u'logodds32', u'logodds33', u'logodds34', u'logodds35', u'logodds36', u'logodds37', u'logodds38', u'Address_Importance_-1', u'Address_Importance_0', u'Address_Importance_1', u'Address_Importance_2', u'Address_Importance_3', u'Address_Importance_4', u'Address_Importance_5', u'Address_Importance_6', u'Address_Importance_7', u'Address_Importance_8', u'Address_Importance_9', u'Address_Importance_10', u'Address_Importance_11', u'Address_Importance_12', u'Address_Importance_13', u'Address_Importance_14', u'Address_Importance_15', u'Address_Importance_16', u'Address_Importance_17', u'Address_Importance_18', u'Address_Importance_19', u'Address_Importance_20', u'Address_Importance_21', u'Address_Importance_22', u'Address_Importance_23', u'Address_Importance_24', u'Address_Importance_25', u'Address_Importance_26', u'Address_Importance_27', u'Address_Importance_28', u'Address_Importance_29', u'Address_Importance_30', u'Address_Importance_31', u'Address_Importance_32', u'Address_Importance_33', u'Address_Importance_34', ...], dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collist = features.columns.tolist()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(features)\n",
    "features[collist] = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_binary = pd.get_dummies(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating address features\n",
      "Parsing dates\n",
      "Creating one-hot variables\n",
      "droping processed columns\n",
      "joining one-hot features\n",
      "creating new features\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test[[\"X\",\"Y\"]] = xy_scaler.transform(test[[\"X\",\"Y\"]])\n",
    "\n",
    "test[\"X\"] = test[\"X\"].apply(lambda x: 0 if abs(x)>5 else x)\n",
    "test[\"Y\"] = test[\"Y\"].apply(lambda y: 0 if abs(y)>5 else y)\n",
    "\n",
    "new_addresses = sorted(test[\"Address\"].unique())\n",
    "new_A_counts=test.groupby(\"Address\").size()\n",
    "only_new = set(new_addresses + addresses) - set(addresses)\n",
    "only_old = set(new_addresses + addresses) - set(new_addresses)\n",
    "in_both = set(new_addresses).intersection(addresses)\n",
    "for addr in only_new:\n",
    "    PA = new_A_counts[addr]/float(len(test) + len(train))\n",
    "    logoddsPA[addr] = np.log(PA) - np.log(1.-PA)\n",
    "    logodds[addr] = deepcopy(default_logodds)\n",
    "    logodds[addr].index = range(len(categories))\n",
    "for addr in in_both:\n",
    "    PA = (A_counts[addr] + new_A_counts[addr])/float(len(test)+len(train))\n",
    "    logoddsPA[addr] = np.log(PA) - np.log(1.-PA)\n",
    "\n",
    "\n",
    "features_sub, _ = parse_data(test, logodds, logoddsPA)\n",
    "\n",
    "collist = features_sub.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_sub[collist] = scaler.transform(features_sub[collist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd = 57\n",
    "param = {'max_depth': 8, \n",
    "         'eta': 0.1, \n",
    "         # next step: eta to 0.15\n",
    "         'silent': 1, \n",
    "         'objective': 'multi:softprob',\n",
    "         'eval_metric': \"mlogloss\",\n",
    "         'min_child_weight': 1,\n",
    "         'subsample': 0.7,\n",
    "         'colsample_bytree': 0.7,\n",
    "         'num_class': 39,\n",
    "         'seed': rnd\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "labelsInt = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 0\n"
     ]
    }
   ],
   "source": [
    "print max(labelsInt), min(labelsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(labelsInt, 1, test_size = 0.15, random_state = 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in split:\n",
    "    trainX = features.as_matrix()[train_index]\n",
    "    trainY = labelsInt[train_index]\n",
    "    cvX = features.as_matrix()[test_index]\n",
    "    cvY = labelsInt[test_index]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(trainX, label = trainY)\n",
    "    dtest = xgb.DMatrix(cvX, label = cvY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watchlist  = [(dtrain,'train'), (dtest,'eval')]\n",
    "numRound = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgbModel = xgb.train(param, dtrain, numRound, watchlist, early_stopping_rounds = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrainFull = xgb.DMatrix(features.as_matrix(), label = labelsInt)\n",
    "watchlist  = [(dtrainFull,'train'), (dtrainFull,'eval')]\n",
    "xgbModel = xgb.train(param, dtrainFull, numRound, watchlist, early_stopping_rounds = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testPred = xgbModel.predict(xgb.DMatrix(features_sub.as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(testPred, columns = sorted(labels.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.GzipFile('SFCrimePredXGBoost.csv.gz',mode = 'w',compresslevel = 9) as gzfile: pred.to_csv(gzfile,index_label = \"Id\",na_rep = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
